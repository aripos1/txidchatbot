"""
ë¹—ì¸ FAQ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì†Œìˆ˜ì˜ ì•„í‹°í´ë§Œ í¬ë¡¤ë§í•˜ì—¬ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""
import asyncio
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from scripts.data.crawl_bithumb import (
    vector_store,
    discover_all_articles,
    extract_article_content,
    store_article_to_vector_db,
    BASE_URL,
    HELP_CENTER_BASE
)
import httpx
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


async def test_crawl(limit: int = 3):
    """í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ (ì†Œìˆ˜ì˜ ì•„í‹°í´ë§Œ)"""
    print("=" * 60)
    print("ë¹—ì¸ FAQ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # MongoDB ì—°ê²°
    print("\n1. MongoDB Atlas ì—°ê²° ì¤‘...")
    connected = await vector_store.connect()
    if not connected:
        print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨. ì—°ê²° ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print("\nğŸ’¡ í™•ì¸ ì‚¬í•­:")
        print("   - .env íŒŒì¼ì— MONGODB_URIê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        print("   - MongoDB Atlas ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ ì„¤ì • í™•ì¸")
        return False
    
    print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
    
    # ì•„í‹°í´ URL ë°œê²¬ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì†Œìˆ˜ë§Œ)
    print(f"\n2. ì•„í‹°í´ URL ë°œê²¬ ì¤‘... (ìµœëŒ€ {limit}ê°œë§Œ í…ŒìŠ¤íŠ¸)")
    print("-" * 60)
    
    # ì¿ í‚¤ì™€ ì„¸ì…˜ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    async with httpx.AsyncClient(
        timeout=30.0, 
        follow_redirects=True,
        cookies={},
        limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
    ) as client:
        # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ì¿ í‚¤ ë°›ê¸°
        try:
            logging.info("ë©”ì¸ í˜ì´ì§€ ì ‘ì†í•˜ì—¬ ì¿ í‚¤ ë°›ëŠ” ì¤‘...")
            from scripts.data.crawl_bithumb import BASE_URL, HEADERS
            await client.get(f"{BASE_URL}/", headers=HEADERS, timeout=30.0)
            await asyncio.sleep(1)
        except Exception as e:
            logging.warning(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        try:
            article_urls = await discover_all_articles(client)
        except Exception as e:
            print(f"âŒ ì•„í‹°í´ ë°œê²¬ ì‹¤íŒ¨: {e}")
            logging.exception("ì•„í‹°í´ ë°œê²¬ ì˜¤ë¥˜")
            await vector_store.disconnect()
            return False
    
    if not article_urls:
        print("âŒ ì•„í‹°í´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        await vector_store.disconnect()
        return False
    
    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ
    test_urls = article_urls[:limit]
    print(f"\n3. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {len(test_urls)}ê°œ ì•„í‹°í´ (ì „ì²´ {len(article_urls)}ê°œ ì¤‘)")
    
    for i, url in enumerate(test_urls, 1):
        print(f"   {i}. {url}")
    
    print("\n4. í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print("-" * 60)
    
    success_count = 0
    fail_count = 0
    
    # ê° ì•„í‹°í´ ì²˜ë¦¬ ë° ì €ì¥ (ì¿ í‚¤ ìœ ì§€)
    async with httpx.AsyncClient(
        timeout=30.0, 
        follow_redirects=True,
        cookies={},
        limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
    ) as client:
        # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ì¿ í‚¤ ë°›ê¸°
        try:
            from scripts.data.crawl_bithumb import BASE_URL, HEADERS
            await client.get(f"{BASE_URL}/", headers=HEADERS, timeout=30.0)
            await asyncio.sleep(1)
        except Exception:
            pass
        for i, article_url in enumerate(test_urls, 1):
            try:
                print(f"\n[{i}/{len(test_urls)}] í¬ë¡¤ë§ ì¤‘: {article_url}")
                
                # ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ
                article_data = await extract_article_content(client, article_url)
                
                if not article_data:
                    fail_count += 1
                    print(f"âš ï¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                    continue
                
                if not article_data.get("body"):
                    fail_count += 1
                    print(f"âš ï¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŒ")
                    continue
                
                title = article_data["title"][:50]
                body_length = len(article_data["body"])
                images_count = len(article_data.get("images", []))
                
                print(f"   ì œëª©: {title}...")
                print(f"   ë³¸ë¬¸ ê¸¸ì´: {body_length}ì")
                print(f"   ì´ë¯¸ì§€ ìˆ˜: {images_count}ê°œ")
                
                if images_count > 0:
                    print(f"   ì´ë¯¸ì§€ ì •ë³´:")
                    for img_idx, img in enumerate(article_data["images"][:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                        img_url = img.get("url", "")[:60]
                        img_alt = img.get("alt", "")[:30]
                        print(f"     {img_idx}. {img_url}... (alt: {img_alt})")
                
                # ë²¡í„° DBì— ì €ì¥
                print(f"   ë²¡í„° DB ì €ì¥ ì¤‘...")
                if await store_article_to_vector_db(article_data):
                    success_count += 1
                    print(f"âœ… ì €ì¥ ì™„ë£Œ: {title[:40]}...")
                else:
                    fail_count += 1
                    print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨")
                
                # Rate limit ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                await asyncio.sleep(0.5)
                
            except Exception as e:
                fail_count += 1
                print(f"âŒ ì‹¤íŒ¨: {article_url}")
                print(f"   ì˜¤ë¥˜: {str(e)[:100]}")
                logging.exception(f"ì•„í‹°í´ ì²˜ë¦¬ ì˜¤ë¥˜: {article_url}")
                continue
    
    print("\n" + "=" * 60)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {fail_count}ê°œ")
    print("=" * 60)
    
    if success_count > 0:
        print("\nâœ… í¬ë¡¤ë§ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ë°œê²¬ëœ ì „ì²´ ì•„í‹°í´: {len(article_urls)}ê°œ")
        print(f"   - í…ŒìŠ¤íŠ¸í•œ ì•„í‹°í´: {len(test_urls)}ê°œ")
        print(f"   - ì„±ê³µì ìœ¼ë¡œ ì €ì¥: {success_count}ê°œ")
        print(f"\nğŸ’¡ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ë ¤ë©´:")
        print(f"   python scripts/data/crawl_bithumb.py")
    else:
        print("\nâŒ í¬ë¡¤ë§ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("   ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
    
    # ì—°ê²° í•´ì œ
    await vector_store.disconnect()
    return success_count > 0


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¹—ì¸ FAQ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸')
    parser.add_argument(
        '--limit',
        type=int,
        default=3,
        help='í…ŒìŠ¤íŠ¸í•  ì•„í‹°í´ ìˆ˜ (ê¸°ë³¸ê°’: 3)'
    )
    
    args = parser.parse_args()
    
    success = asyncio.run(test_crawl(limit=args.limit))
    sys.exit(0 if success else 1)
