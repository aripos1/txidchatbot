"""
ë¹—ì¸ ê³ ê°ì§€ì› í˜ì´ì§€ í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ ìŠ¤í¬ë¦½íŠ¸
Zendesk Help Center ì›¹ í¬ë¡¤ë§ì„ í†µí•´ FAQë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Set, Optional
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from chatbot.vector_store import vector_store
import logging
import httpx
from bs4 import BeautifulSoup
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Zendesk Help Center ì„¤ì •
BASE_URL = "https://support.bithumb.com"
LOCALE = "ko"
HELP_CENTER_BASE = f"{BASE_URL}/hc/{LOCALE}"

# ìš”ì²­ í—¤ë” (ë´‡ ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•´ ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ì„¤ì •)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
    "Referer": "https://www.bithumb.com/",
}


async def fetch_page(client: httpx.AsyncClient, url: str, referer: str = None) -> Optional[BeautifulSoup]:
    """ì›¹ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ BeautifulSoup ê°ì²´ë¡œ ë°˜í™˜"""
    try:
        # Referer í—¤ë” ì¶”ê°€ (403 ì˜¤ë¥˜ ë°©ì§€)
        headers = HEADERS.copy()
        if referer:
            headers["Referer"] = referer
        elif BASE_URL in url:
            headers["Referer"] = f"{BASE_URL}/"
        
        # ì¿ í‚¤ì™€ ì„¸ì…˜ ìœ ì§€ë¥¼ ìœ„í•œ ì„¤ì •
        response = await client.get(
            url, 
            headers=headers, 
            timeout=30.0, 
            follow_redirects=True,
            cookies=client.cookies  # ì¿ í‚¤ ìœ ì§€
        )
        
        # 403 ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„ (ë” ê¸´ ëŒ€ê¸° ì‹œê°„)
        if response.status_code == 403:
            logging.warning(f"403 ì˜¤ë¥˜ ë°œìƒ, 3ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„: {url}")
            await asyncio.sleep(3)
            
            # User-Agentë¥¼ ì•½ê°„ ë³€ê²½í•˜ì—¬ ì¬ì‹œë„
            retry_headers = headers.copy()
            retry_headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            
            response = await client.get(
                url,
                headers=retry_headers,
                timeout=30.0,
                follow_redirects=True,
                cookies=client.cookies
            )
        
        response.raise_for_status()
        return BeautifulSoup(response.text, 'html.parser')
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 403:
            logging.error(f"403 Forbidden ì˜¤ë¥˜ ({url}) - ì›¹ì‚¬ì´íŠ¸ê°€ í¬ë¡¤ëŸ¬ë¥¼ ì°¨ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            logging.error("í•´ê²° ë°©ë²•:")
            logging.error("1. ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì ‘ì†í•˜ì—¬ í™•ì¸")
            logging.error("2. User-Agentë¥¼ ì—…ë°ì´íŠ¸")
            logging.error("3. ì¿ í‚¤/ì„¸ì…˜ ì„¤ì • í™•ì¸")
        else:
            logging.warning(f"HTTP ì˜¤ë¥˜ ({url}): {e.response.status_code}")
        return None
    except Exception as e:
        logging.warning(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
        return None


async def discover_categories(client: httpx.AsyncClient) -> Set[str]:
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ URL ë°œê²¬"""
    categories = set()
    
    # ë©”ì¸ í˜ì´ì§€ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§í¬ ì°¾ê¸° (Referer ì¶”ê°€)
    soup = await fetch_page(client, f"{HELP_CENTER_BASE}", referer="https://www.bithumb.com/")
    if not soup:
        return categories
    
    # ì¹´í…Œê³ ë¦¬ ë§í¬ ì°¾ê¸°
    category_links = soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/categories/\d+'))
    
    for link in category_links:
        href = link.get('href', '')
        if href:
            if href.startswith('/'):
                full_url = f"{BASE_URL}{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue
            
            if '/categories/' in full_url:
                categories.add(full_url)
    
    logging.info(f"ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories)}")
    return categories


async def discover_sections(client: httpx.AsyncClient, category_url: str) -> Set[str]:
    """ì¹´í…Œê³ ë¦¬ì—ì„œ ëª¨ë“  ì„¹ì…˜ URL ë°œê²¬"""
    sections = set()
    
    soup = await fetch_page(client, category_url)
    if not soup:
        return sections
    
    # ì„¹ì…˜ ë§í¬ ì°¾ê¸°
    section_links = soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/sections/\d+'))
    
    for link in section_links:
        href = link.get('href', '')
        if href:
            if href.startswith('/'):
                full_url = f"{BASE_URL}{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue
            
            if '/sections/' in full_url:
                sections.add(full_url)
    
    return sections


async def discover_articles_from_section(client: httpx.AsyncClient, section_url: str) -> Set[str]:
    """ì„¹ì…˜ì—ì„œ ëª¨ë“  ì•„í‹°í´ URL ë°œê²¬"""
    articles = set()
    
    soup = await fetch_page(client, section_url)
    if not soup:
        return articles
    
    # ì•„í‹°í´ ë§í¬ ì°¾ê¸°
    article_links = soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/articles/\d+'))
    
    for link in article_links:
        href = link.get('href', '')
        if href:
            if href.startswith('/'):
                full_url = f"{BASE_URL}{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue
            
            if '/articles/' in full_url:
                articles.add(full_url)
    
    return articles


async def discover_all_articles(client: httpx.AsyncClient) -> List[str]:
    """ëª¨ë“  ì•„í‹°í´ URLì„ ì²´ê³„ì ìœ¼ë¡œ ë°œê²¬"""
    all_articles = set()
    
    # 1. ì¹´í…Œê³ ë¦¬ ë°œê²¬
    logging.info("ì¹´í…Œê³ ë¦¬ ë°œê²¬ ì¤‘...")
    categories = await discover_categories(client)
    
    # 2. ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ì„¹ì…˜ ë°œê²¬
    all_sections = set()
    for category_url in categories:
        logging.info(f"ì¹´í…Œê³ ë¦¬ì—ì„œ ì„¹ì…˜ ë°œê²¬ ì¤‘: {category_url}")
        sections = await discover_sections(client, category_url)
        all_sections.update(sections)
        await asyncio.sleep(0.3)  # Rate limit ë°©ì§€
    
    logging.info(f"ë°œê²¬ëœ ì„¹ì…˜ ìˆ˜: {len(all_sections)}")
    
    # 3. ê° ì„¹ì…˜ì—ì„œ ì•„í‹°í´ ë°œê²¬
    for section_url in all_sections:
        logging.info(f"ì„¹ì…˜ì—ì„œ ì•„í‹°í´ ë°œê²¬ ì¤‘: {section_url}")
        articles = await discover_articles_from_section(client, section_url)
        all_articles.update(articles)
        await asyncio.sleep(0.3)  # Rate limit ë°©ì§€
    
    # 4. ë©”ì¸ í˜ì´ì§€ì—ì„œë„ ì§ì ‘ ì•„í‹°í´ ë§í¬ ì°¾ê¸°
    logging.info("ë©”ì¸ í˜ì´ì§€ì—ì„œ ì•„í‹°í´ ë°œê²¬ ì¤‘...")
    main_soup = await fetch_page(client, f"{HELP_CENTER_BASE}")
    if main_soup:
        main_article_links = main_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/articles/\d+'))
        for link in main_article_links:
            href = link.get('href', '')
            if href:
                if href.startswith('/'):
                    full_url = f"{BASE_URL}{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                if '/articles/' in full_url:
                    all_articles.add(full_url)
    
    logging.info(f"ì´ ë°œê²¬ëœ ì•„í‹°í´ ìˆ˜: {len(all_articles)}")
    return list(all_articles)


def extract_images_from_element(elem) -> List[Dict]:
    """ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
    images = []
    
    if not elem:
        return images
    
    # ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
    img_tags = elem.find_all('img')
    
    for img in img_tags:
        img_info = {}
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
        if img_url:
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_url.startswith('//'):
                img_url = f"https:{img_url}"
            elif img_url.startswith('/'):
                img_url = f"{BASE_URL}{img_url}"
            elif not img_url.startswith('http'):
                continue
            
            img_info['url'] = img_url
        
        # Alt í…ìŠ¤íŠ¸ ì¶”ì¶œ
        alt_text = img.get('alt', '').strip()
        if alt_text:
            img_info['alt'] = alt_text
        
        # Title ì†ì„± ì¶”ì¶œ
        title_text = img.get('title', '').strip()
        if title_text:
            img_info['title'] = title_text
        
        # ì´ë¯¸ì§€ ì£¼ë³€ í…ìŠ¤íŠ¸ (ìº¡ì…˜, ì„¤ëª…) ì¶”ì¶œ
        # ë¶€ëª¨ ìš”ì†Œì—ì„œ figcaption, caption í´ë˜ìŠ¤ ì°¾ê¸°
        parent = img.find_parent(['figure', 'div', 'p'])
        if parent:
            caption = parent.find(class_=re.compile(r'caption|figcaption|image.*caption', re.I))
            if caption:
                caption_text = caption.get_text(strip=True)
                if caption_text:
                    img_info['caption'] = caption_text
            
            # ì´ë¯¸ì§€ ì•ë’¤ í…ìŠ¤íŠ¸ë„ í¬í•¨ (ì„¤ëª…ì¼ ìˆ˜ ìˆìŒ)
            img_text_parts = []
            
            # ì´ë¯¸ì§€ ì• í…ìŠ¤íŠ¸
            prev_sibling = img.find_previous_sibling(['p', 'div', 'span'])
            if prev_sibling:
                prev_text = prev_sibling.get_text(strip=True)
                if prev_text and len(prev_text) < 200:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    img_text_parts.append(prev_text)
            
            # ì´ë¯¸ì§€ ë’¤ í…ìŠ¤íŠ¸
            next_sibling = img.find_next_sibling(['p', 'div', 'span'])
            if next_sibling:
                next_text = next_sibling.get_text(strip=True)
                if next_text and len(next_text) < 200:
                    img_text_parts.append(next_text)
            
            if img_text_parts:
                img_info['context'] = ' '.join(img_text_parts)
        
        if img_info:
            images.append(img_info)
    
    return images


async def extract_article_content(client: httpx.AsyncClient, article_url: str) -> Optional[Dict]:
    """ì•„í‹°í´ í˜ì´ì§€ì—ì„œ ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ ì¶”ì¶œ"""
    soup = await fetch_page(client, article_url)
    if not soup:
        return None
    
    try:
        # ì œëª© ì¶”ì¶œ (ì¼ë°˜ì ìœ¼ë¡œ h1 íƒœê·¸ ë˜ëŠ” article-title í´ë˜ìŠ¤)
        title_elem = soup.find('h1') or soup.find(class_=re.compile(r'article.*title|title.*article', re.I))
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ ì¶”ì¶œ (ì¼ë°˜ì ìœ¼ë¡œ article-body í´ë˜ìŠ¤ ë˜ëŠ” article íƒœê·¸)
        body_elem = (
            soup.find(class_=re.compile(r'article.*body|body.*article', re.I)) or
            soup.find('article') or
            soup.find(id=re.compile(r'article.*content|content.*article', re.I))
        )
        
        images = []
        body_text = ""
        
        if body_elem:
            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (íƒœê·¸ ì œê±° ì „ì— ìˆ˜í–‰)
            images = extract_images_from_element(body_elem)
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in body_elem(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                tag.decompose()
            
            body_text = body_elem.get_text(separator='\n', strip=True)
        else:
            # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|main', re.I))
            if main_content:
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                images = extract_images_from_element(main_content)
                
                for tag in main_content(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    tag.decompose()
                body_text = main_content.get_text(separator='\n', strip=True)
            else:
                # ì „ì²´ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                images = extract_images_from_element(soup)
                body_text = soup.get_text(separator='\n', strip=True)
        
        # ì—¬ëŸ¬ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        lines = [line.strip() for line in body_text.split('\n') if line.strip()]
        clean_body = '\n'.join(lines)
        
        # ì´ë¯¸ì§€ ì„¤ëª…ì„ í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°)
        image_descriptions = []
        for img in images:
            img_desc_parts = []
            
            if img.get('alt'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ì„¤ëª…: {img['alt']}]")
            
            if img.get('caption'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ìº¡ì…˜: {img['caption']}]")
            
            if img.get('context'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ì£¼ë³€ ì„¤ëª…: {img['context']}]")
            
            if img_desc_parts:
                image_descriptions.append(' '.join(img_desc_parts))
        
        # ì´ë¯¸ì§€ ì„¤ëª…ì„ ë³¸ë¬¸ì— ì¶”ê°€
        if image_descriptions:
            clean_body += "\n\n" + "\n".join(image_descriptions)
        
        # ì•„í‹°í´ ID ì¶”ì¶œ
        article_id_match = re.search(r'/articles/(\d+)', article_url)
        article_id = article_id_match.group(1) if article_id_match else None
        
        return {
            "url": article_url,
            "title": title,
            "body": clean_body,
            "article_id": article_id,
            "images": images,  # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            "full_text": f"ì œëª©: {title}\n\n{clean_body}"
        }
        
    except Exception as e:
        logging.error(f"ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {e}")
        return None


async def store_article_to_vector_db(article_data: Dict):
    """ì•„í‹°í´ì„ ë²¡í„° DBì— ì €ì¥"""
    if vector_store.collection is None:
        logging.error("MongoDBê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        text = article_data["full_text"]
        chunks = vector_store.split_text(text, chunk_size=1000, overlap=200)
        
        stored_count = 0
        for i, chunk in enumerate(chunks):
            try:
                # ì„ë² ë”© ìƒì„±
                embedding = await vector_store.create_embedding(chunk)
                if not embedding:
                    continue
                
                # ë¬¸ì„œ ID ìƒì„±
                import hashlib
                doc_id = hashlib.md5(
                    f"zendesk_{article_data['article_id']}_{i}".encode()
                ).hexdigest()
                
                # ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
                metadata = {
                    "article_id": article_data.get("article_id"),
                    "title": article_data["title"],
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "type": "zendesk_article",
                    "created_at": datetime.utcnow().isoformat()
                }
                
                # ì²­í¬ì— ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê²½ìš° ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
                # (ì´ë¯¸ì§€ ì„¤ëª…ì´ ì²­í¬ì— í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
                if article_data.get("images"):
                    # í˜„ì¬ ì²­í¬ì˜ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸°
                    chunk_start = sum(len(chunks[j]) for j in range(i))
                    chunk_end = chunk_start + len(chunk)
                    
                    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ì„¤ëª…ì˜ ìœ„ì¹˜ë¥¼ ì¶”ì •
                    # (ê°„ë‹¨í•œ ë°©ë²•: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨)
                    if i == 0:  # ì²« ë²ˆì§¸ ì²­í¬ì—ë§Œ ì´ë¯¸ì§€ ì •ë³´ í¬í•¨ (ì¤‘ë³µ ë°©ì§€)
                        metadata["images"] = article_data["images"]
                
                # MongoDBì— ì €ì¥
                document = {
                    "_id": doc_id,
                    "text": chunk,
                    "source": article_data["url"],
                    "metadata": metadata,
                    "embedding": embedding,
                    "created_at": datetime.utcnow()
                }
                
                await vector_store.collection.update_one(
                    {"_id": doc_id},
                    {"$set": document},
                    upsert=True
                )
                
                stored_count += 1
                
            except Exception as e:
                logging.error(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨ (ì•„í‹°í´ {article_data.get('article_id')}, ì²­í¬ {i}): {e}")
                continue
        
        logging.info(f"ì•„í‹°í´ {article_data.get('article_id')} ì €ì¥ ì™„ë£Œ: {stored_count}/{len(chunks)} ì²­í¬")
        return stored_count > 0
        
    except Exception as e:
        logging.error(f"ì•„í‹°í´ ì €ì¥ ì‹¤íŒ¨ ({article_data.get('article_id')}): {e}")
        return False


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ë¹—ì¸ ê³ ê°ì§€ì› ì„¼í„° FAQ í¬ë¡¤ë§ (ì›¹ í¬ë¡¤ë§)")
    print("=" * 60)
    
    # MongoDB ì—°ê²°
    print("\n1. MongoDB Atlas ì—°ê²° ì¤‘...")
    connected = await vector_store.connect()
    if not connected:
        print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨. ì—°ê²° ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
    
    # ì•„í‹°í´ URL ë°œê²¬
    print("\n2. ì•„í‹°í´ URL ë°œê²¬ ì¤‘...")
    print("   (ì¹´í…Œê³ ë¦¬ â†’ ì„¹ì…˜ â†’ ì•„í‹°í´ ìˆœì„œë¡œ íƒìƒ‰)")
    print("   (ì´ ì‘ì—…ì€ ëª‡ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")
    print("-" * 60)
    
    # ì¿ í‚¤ì™€ ì„¸ì…˜ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    async with httpx.AsyncClient(
        timeout=30.0, 
        follow_redirects=True,
        cookies={},  # ì¿ í‚¤ ì €ì¥ì†Œ ì´ˆê¸°í™”
        limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
    ) as client:
        # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ì¿ í‚¤ ë°›ê¸°
        try:
            logging.info("ë©”ì¸ í˜ì´ì§€ ì ‘ì†í•˜ì—¬ ì¿ í‚¤ ë°›ëŠ” ì¤‘...")
            await client.get(f"{BASE_URL}/", headers=HEADERS, timeout=30.0)
            await asyncio.sleep(1)  # ì¿ í‚¤ ì„¤ì • ëŒ€ê¸°
        except Exception as e:
            logging.warning(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        article_urls = await discover_all_articles(client)
    
    if not article_urls:
        print("âŒ ì•„í‹°í´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        await vector_store.disconnect()
        return
    
    print(f"\n3. ì´ {len(article_urls)}ê°œ ì•„í‹°í´ ë°œê²¬")
    print("   ì•„í‹°í´ ë‚´ìš© í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ ì‹œì‘...")
    print("-" * 60)
    
    success_count = 0
    fail_count = 0
    
    # ê° ì•„í‹°í´ ì²˜ë¦¬ ë° ì €ì¥ (ì¿ í‚¤ ìœ ì§€)
    async with httpx.AsyncClient(
        timeout=30.0, 
        follow_redirects=True,
        cookies={},
        limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
    ) as client:
        # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ì¿ í‚¤ ë°›ê¸°
        try:
            await client.get(f"{BASE_URL}/", headers=HEADERS, timeout=30.0)
            await asyncio.sleep(1)
        except Exception:
            pass
        
        for i, article_url in enumerate(article_urls, 1):
            try:
                print(f"\n[{i}/{len(article_urls)}] í¬ë¡¤ë§ ì¤‘: {article_url}")
                
                # ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ
                article_data = await extract_article_content(client, article_url)
                
                if not article_data or not article_data.get("body"):
                    fail_count += 1
                    print(f"âš ï¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {article_url}")
                    continue
                
                title = article_data["title"][:50]
                print(f"   ì œëª©: {title}...")
                
                # ë²¡í„° DBì— ì €ì¥
                if await store_article_to_vector_db(article_data):
                    success_count += 1
                    print(f"âœ… ì™„ë£Œ: {title[:40]}...")
                else:
                    fail_count += 1
                    print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {title[:40]}...")
                
                # Rate limit ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                await asyncio.sleep(0.5)
                
            except Exception as e:
                fail_count += 1
                print(f"âŒ ì‹¤íŒ¨: {article_url} - {e}")
                logging.exception(f"ì•„í‹°í´ ì²˜ë¦¬ ì˜¤ë¥˜: {article_url}")
                continue
    
    print("\n" + "=" * 60)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {fail_count}ê°œ")
    print("=" * 60)
    
    print("\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. MongoDB Atlasì—ì„œ ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸")
    print("2. FAQ Specialistì—ì„œ DB ê²€ìƒ‰ì„ í™œì„±í™”í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ ê°€ëŠ¥")
    print("3. ì •ê¸°ì ìœ¼ë¡œ FAQ í˜ì´ì§€ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ìµœì‹  ì •ë³´ ìœ ì§€")
    
    # ì—°ê²° í•´ì œ
    await vector_store.disconnect()

if __name__ == "__main__":
    asyncio.run(main())

