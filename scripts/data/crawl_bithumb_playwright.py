"""
ë¹—ì¸ FAQ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ (Playwright ì‚¬ìš©)
Cloudflare ë³´í˜¸ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
PlaywrightëŠ” Seleniumë³´ë‹¤ ë¹ ë¥´ê³  ì•ˆì •ì ì…ë‹ˆë‹¤.
"""
import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Optional, Set
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from chatbot.vector_store import vector_store
import logging
from bs4 import BeautifulSoup
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Playwright ì„¤ì •
try:
    from playwright.async_api import async_playwright, Browser, Page, TimeoutError as PlaywrightTimeoutError
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logging.error("Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install playwright && playwright install chromium ì„¤ì¹˜ í•„ìš”")

# Zendesk Help Center ì„¤ì •
BASE_URL = "https://support.bithumb.com"
LOCALE = "ko"
HELP_CENTER_BASE = f"{BASE_URL}/hc/{LOCALE}"


def extract_images_from_element(soup: BeautifulSoup) -> List[Dict]:
    """ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
    images = []
    
    if not soup:
        return images
    
    # ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
    img_tags = soup.find_all('img')
    
    for img in img_tags:
        img_info = {}
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
        if img_url:
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_url.startswith('//'):
                img_url = f"https:{img_url}"
            elif img_url.startswith('/'):
                img_url = f"{BASE_URL}{img_url}"
            elif not img_url.startswith('http'):
                continue
            
            img_info['url'] = img_url
        
        # Alt í…ìŠ¤íŠ¸ ì¶”ì¶œ
        alt_text = img.get('alt', '').strip()
        if alt_text:
            img_info['alt'] = alt_text
        
        # Title ì†ì„± ì¶”ì¶œ
        title_text = img.get('title', '').strip()
        if title_text:
            img_info['title'] = title_text
        
        # ì´ë¯¸ì§€ ì£¼ë³€ í…ìŠ¤íŠ¸ (ìº¡ì…˜, ì„¤ëª…) ì¶”ì¶œ
        parent = img.find_parent(['figure', 'div', 'p'])
        if parent:
            caption = parent.find(class_=re.compile(r'caption|figcaption|image.*caption', re.I))
            if caption:
                caption_text = caption.get_text(strip=True)
                if caption_text:
                    img_info['caption'] = caption_text
            
            # ì´ë¯¸ì§€ ì•ë’¤ í…ìŠ¤íŠ¸ë„ í¬í•¨
            img_text_parts = []
            
            prev_sibling = img.find_previous_sibling(['p', 'div', 'span'])
            if prev_sibling:
                prev_text = prev_sibling.get_text(strip=True)
                if prev_text and len(prev_text) < 200:
                    img_text_parts.append(prev_text)
            
            next_sibling = img.find_next_sibling(['p', 'div', 'span'])
            if next_sibling:
                next_text = next_sibling.get_text(strip=True)
                if next_text and len(next_text) < 200:
                    img_text_parts.append(next_text)
            
            if img_text_parts:
                img_info['context'] = ' '.join(img_text_parts)
        
        if img_info:
            images.append(img_info)
    
    return images


async def discover_all_articles_playwright(page: Page, limit: Optional[int] = None) -> List[str]:
    """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì•„í‹°í´ URL ë°œê²¬"""
    all_articles = set()
    
    try:
        logging.info("ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
        await page.goto(f"{HELP_CENTER_BASE}", wait_until="networkidle", timeout=30000)
        
        # Cloudflare ì²´í¬ ëŒ€ê¸° (í•„ìš”ì‹œ)
        await asyncio.sleep(2)
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        page_source = await page.content()
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ì¹´í…Œê³ ë¦¬ ë§í¬ ì°¾ê¸°
        category_links = soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/categories/\d+'))
        categories = set()
        for link in category_links:
            href = link.get('href', '')
            if href:
                if href.startswith('/'):
                    full_url = f"{BASE_URL}{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                if '/categories/' in full_url:
                    categories.add(full_url)
        
        logging.info(f"ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories)}")
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ì„¹ì…˜ ì°¾ê¸°
        all_sections = set()
        for category_url in list(categories)[:10]:  # ì²˜ìŒ 10ê°œë§Œ (í…ŒìŠ¤íŠ¸ìš©)
            try:
                logging.info(f"ì¹´í…Œê³ ë¦¬ ì ‘ì†: {category_url}")
                await page.goto(category_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(1)
                
                cat_soup = BeautifulSoup(await page.content(), 'html.parser')
                section_links = cat_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/sections/\d+'))
                
                for link in section_links:
                    href = link.get('href', '')
                    if href:
                        if href.startswith('/'):
                            full_url = f"{BASE_URL}{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        if '/sections/' in full_url:
                            all_sections.add(full_url)
            except Exception as e:
                logging.warning(f"ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({category_url}): {e}")
                continue
        
        logging.info(f"ë°œê²¬ëœ ì„¹ì…˜ ìˆ˜: {len(all_sections)}")
        
        # ê° ì„¹ì…˜ì—ì„œ ì•„í‹°í´ ì°¾ê¸°
        for section_url in list(all_sections)[:20]:  # ì²˜ìŒ 20ê°œë§Œ (í…ŒìŠ¤íŠ¸ìš©)
            try:
                logging.info(f"ì„¹ì…˜ ì ‘ì†: {section_url}")
                await page.goto(section_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(1)
                
                section_soup = BeautifulSoup(await page.content(), 'html.parser')
                article_links = section_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/articles/\d+'))
                
                for link in article_links:
                    href = link.get('href', '')
                    if href:
                        if href.startswith('/'):
                            full_url = f"{BASE_URL}{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        if '/articles/' in full_url:
                            all_articles.add(full_url)
                            
                        if limit and len(all_articles) >= limit:
                            break
                
                if limit and len(all_articles) >= limit:
                    break
            except Exception as e:
                logging.warning(f"ì„¹ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({section_url}): {e}")
                continue
        
        # ë©”ì¸ í˜ì´ì§€ì—ì„œë„ ì§ì ‘ ì•„í‹°í´ ë§í¬ ì°¾ê¸°
        await page.goto(f"{HELP_CENTER_BASE}", wait_until="networkidle", timeout=30000)
        await asyncio.sleep(1)
        main_soup = BeautifulSoup(await page.content(), 'html.parser')
        main_article_links = main_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/articles/\d+'))
        for link in main_article_links:
            href = link.get('href', '')
            if href:
                if href.startswith('/'):
                    full_url = f"{BASE_URL}{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                if '/articles/' in full_url:
                    all_articles.add(full_url)
        
        logging.info(f"ì´ ë°œê²¬ëœ ì•„í‹°í´ ìˆ˜: {len(all_articles)}")
        return list(all_articles)
        
    except Exception as e:
        logging.error(f"ì•„í‹°í´ ë°œê²¬ ì‹¤íŒ¨: {e}")
        return []


async def extract_article_content_playwright(page: Page, article_url: str) -> Optional[Dict]:
    """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ"""
    try:
        logging.info(f"ì•„í‹°í´ ì ‘ì†: {article_url}")
        await page.goto(article_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(1)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        
        page_source = await page.content()
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1') or soup.find(class_=re.compile(r'article.*title|title.*article', re.I))
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        body_elem = (
            soup.find(class_=re.compile(r'article.*body|body.*article', re.I)) or
            soup.find('article') or
            soup.find(id=re.compile(r'article.*content|content.*article', re.I))
        )
        
        images = []
        body_text = ""
        
        if body_elem:
            images = extract_images_from_element(body_elem)
            for tag in body_elem(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                tag.decompose()
            body_text = body_elem.get_text(separator='\n', strip=True)
        else:
            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|main', re.I))
            if main_content:
                images = extract_images_from_element(main_content)
                for tag in main_content(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    tag.decompose()
                body_text = main_content.get_text(separator='\n', strip=True)
            else:
                images = extract_images_from_element(soup)
                body_text = soup.get_text(separator='\n', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = [line.strip() for line in body_text.split('\n') if line.strip()]
        clean_body = '\n'.join(lines)
        
        # ì´ë¯¸ì§€ ì„¤ëª… ì¶”ê°€
        image_descriptions = []
        for img in images:
            img_desc_parts = []
            if img.get('alt'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ì„¤ëª…: {img['alt']}]")
            if img.get('caption'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ìº¡ì…˜: {img['caption']}]")
            if img.get('context'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ì£¼ë³€ ì„¤ëª…: {img['context']}]")
            if img_desc_parts:
                image_descriptions.append(' '.join(img_desc_parts))
        
        if image_descriptions:
            clean_body += "\n\n" + "\n".join(image_descriptions)
        
        # ì•„í‹°í´ ID ì¶”ì¶œ
        article_id_match = re.search(r'/articles/(\d+)', article_url)
        article_id = article_id_match.group(1) if article_id_match else None
        
        return {
            "url": article_url,
            "title": title,
            "body": clean_body,
            "article_id": article_id,
            "images": images,
            "full_text": f"ì œëª©: {title}\n\n{clean_body}"
        }
        
    except Exception as e:
        logging.error(f"ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {e}")
        return None


async def store_article_to_vector_db(article_data: Dict):
    """ì•„í‹°í´ì„ ë²¡í„° DBì— ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©)"""
    from scripts.data.crawl_bithumb import store_article_to_vector_db as base_store
    return await base_store(article_data)


async def main(limit: Optional[int] = None, headless: bool = True):
    """ë©”ì¸ í•¨ìˆ˜"""
    if not PLAYWRIGHT_AVAILABLE:
        print("âŒ Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ë°©ë²•:")
        print("  1. pip install playwright")
        print("  2. playwright install chromium")
        return
    
    print("=" * 60)
    print("ë¹—ì¸ FAQ í¬ë¡¤ë§ (Playwright ì‚¬ìš©)")
    print("=" * 60)
    
    # MongoDB ì—°ê²°
    print("\n1. MongoDB Atlas ì—°ê²° ì¤‘...")
    connected = await vector_store.connect()
    if not connected:
        print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨.")
        return
    
    print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
    
    # Playwright ë¸Œë¼ìš°ì € ì‹œì‘
    print("\n2. ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
    async with async_playwright() as p:
        try:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰ (ë´‡ ê°ì§€ ë°©ì§€ ì˜µì…˜ í¬í•¨)
            browser = await p.chromium.launch(
                headless=headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë´‡ ê°ì§€ ë°©ì§€)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='ko-KR',
                timezone_id='Asia/Seoul',
            )
            
            # ë´‡ ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                window.chrome = {
                    runtime: {}
                };
            """)
            
            page = await context.new_page()
            print("âœ… ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ!")
            
            try:
                # ì•„í‹°í´ URL ë°œê²¬
                print("\n3. ì•„í‹°í´ URL ë°œê²¬ ì¤‘...")
                print("-" * 60)
                article_urls = await discover_all_articles_playwright(page, limit=limit)
                
                if not article_urls:
                    print("âŒ ì•„í‹°í´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                
                if limit:
                    article_urls = article_urls[:limit]
                
                print(f"\n4. ì´ {len(article_urls)}ê°œ ì•„í‹°í´ ë°œê²¬")
                print("   í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ ì‹œì‘...")
                print("-" * 60)
                
                success_count = 0
                fail_count = 0
                
                # ê° ì•„í‹°í´ ì²˜ë¦¬ ë° ì €ì¥
                for i, article_url in enumerate(article_urls, 1):
                    try:
                        print(f"\n[{i}/{len(article_urls)}] í¬ë¡¤ë§ ì¤‘: {article_url}")
                        
                        # ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ
                        article_data = await extract_article_content_playwright(page, article_url)
                        
                        if not article_data or not article_data.get("body"):
                            fail_count += 1
                            print(f"âš ï¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                            continue
                        
                        title = article_data["title"][:50]
                        body_length = len(article_data["body"])
                        images_count = len(article_data.get("images", []))
                        
                        print(f"   ì œëª©: {title}...")
                        print(f"   ë³¸ë¬¸ ê¸¸ì´: {body_length}ì")
                        print(f"   ì´ë¯¸ì§€ ìˆ˜: {images_count}ê°œ")
                        
                        if images_count > 0:
                            print(f"   ì´ë¯¸ì§€ ì •ë³´:")
                            for img_idx, img in enumerate(article_data["images"][:3], 1):
                                img_url = img.get("url", "")[:60]
                                img_alt = img.get("alt", "")[:30]
                                print(f"     {img_idx}. {img_url}... (alt: {img_alt})")
                        
                        # ë²¡í„° DBì— ì €ì¥
                        print(f"   ë²¡í„° DB ì €ì¥ ì¤‘...")
                        if await store_article_to_vector_db(article_data):
                            success_count += 1
                            print(f"âœ… ì €ì¥ ì™„ë£Œ: {title[:40]}...")
                        else:
                            fail_count += 1
                            print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨")
                        
                        await asyncio.sleep(1)  # Rate limit ë°©ì§€
                        
                    except Exception as e:
                        fail_count += 1
                        print(f"âŒ ì‹¤íŒ¨: {article_url} - {e}")
                        logging.exception(f"ì•„í‹°í´ ì²˜ë¦¬ ì˜¤ë¥˜: {article_url}")
                        continue
                
                print("\n" + "=" * 60)
                print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
                print(f"   ì„±ê³µ: {success_count}ê°œ")
                print(f"   ì‹¤íŒ¨: {fail_count}ê°œ")
                print("=" * 60)
                
                if success_count > 0:
                    print("\nâœ… í¬ë¡¤ë§ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
                    print(f"\nğŸ“Š ê²°ê³¼:")
                    print(f"   - ë°œê²¬ëœ ì•„í‹°í´: {len(article_urls)}ê°œ")
                    print(f"   - ì„±ê³µì ìœ¼ë¡œ ì €ì¥: {success_count}ê°œ")
                    print(f"\nğŸ’¡ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ë ¤ë©´:")
                    print(f"   python scripts/data/crawl_bithumb_playwright.py")
                
            finally:
                await page.close()
                await context.close()
                await browser.close()
                print("\në¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        
        except Exception as e:
            logging.error(f"ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            print(f"âŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    # MongoDB ì—°ê²° í•´ì œ
    await vector_store.disconnect()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¹—ì¸ FAQ í¬ë¡¤ë§ (Playwright ì‚¬ìš©)')
    parser.add_argument('--limit', type=int, default=None, help='í¬ë¡¤ë§í•  ì•„í‹°í´ ìˆ˜ ì œí•œ')
    parser.add_argument('--no-headless', action='store_true', help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™” (ë¸Œë¼ìš°ì € í‘œì‹œ)')
    
    args = parser.parse_args()
    
    asyncio.run(main(limit=args.limit, headless=not args.no_headless))
