"""
Researcher ë…¸ë“œ - ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
"""
import re
import sys
import logging
import asyncio
from datetime import datetime, timezone, timedelta
import httpx
from langchain_core.messages import HumanMessage, AIMessage
from langsmith import traceable

from ...models import ChatState
from ...configuration import config
from ...utils import ensure_logger_setup

# ì„ íƒì  ì˜ì¡´ì„± (DuckDuckGo)
try:
    from ddgs import DDGS
except ImportError:
    try:
        from duckduckgo_search import DDGS
    except ImportError:
        DDGS = None

logger = logging.getLogger(__name__)


async def _search_with_google(search_queries: list) -> tuple[list, bool]:
    """Google Custom Search APIë¡œ ê²€ìƒ‰"""
    google_api_key = config.GOOGLE_API_KEY
    google_cx = config.GOOGLE_CX
    
    if not google_api_key or not google_cx:
        return [], False
    
    all_results = []
    seen_urls = set()
    rate_limit_hit = False
    
    logger.info(f"Google ê²€ìƒ‰ ì‹œì‘: {len(search_queries)}ê°œ ì¿¼ë¦¬")
    
    async with httpx.AsyncClient(timeout=15.0) as client:
        for query_idx, query in enumerate(search_queries, 1):
            if rate_limit_hit:
                break
                
            try:
                if query_idx > 1:
                    await asyncio.sleep(0.5)
                
                url = config.GOOGLE_SEARCH_API_URL
                params = {
                    "key": google_api_key,
                    "cx": google_cx,
                    "q": query,
                    "num": min(config.MAX_RESULTS_PER_QUERY, 10),
                    "lr": "lang_ko",
                }
                
                response = await client.get(url, params=params)
                
                if response.status_code == 429:
                    logger.warning("Google API í• ë‹¹ëŸ‰ ì´ˆê³¼")
                    rate_limit_hit = True
                    break
                
                if response.status_code != 200:
                    continue
                
                data = response.json()
                
                if "error" in data:
                    error_code = data["error"].get("code", 0)
                    if error_code == 429:
                        rate_limit_hit = True
                        break
                    continue
                
                if "items" in data:
                    for item in data.get("items", []):
                        url_link = item.get("link", "")
                        if url_link and url_link not in seen_urls:
                            seen_urls.add(url_link)
                            all_results.append({
                                "title": item.get("title", ""),
                                "snippet": item.get("snippet", ""),
                                "url": url_link,
                            })
                            
            except httpx.TimeoutException:
                logger.warning(f"Google ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query[:50]}")
            except Exception as e:
                logger.error(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
    
    logger.info(f"Google ê²€ìƒ‰ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼")
    return all_results, rate_limit_hit


async def _search_with_duckduckgo(search_queries: list) -> list:
    """DuckDuckGoë¡œ ê²€ìƒ‰"""
    if DDGS is None:
        logger.warning("DuckDuckGo ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
        return []
    
    all_results = []
    seen_urls = set()
    
    logger.info(f"DuckDuckGo ê²€ìƒ‰ ì‹œì‘: {len(search_queries)}ê°œ ì¿¼ë¦¬")
    
    # site: ì¿¼ë¦¬ ë³€í™˜
    processed_queries = []
    for query in search_queries:
        if 'site:bithumb.com' in query.lower():
            cleaned_query = query.lower().replace('site:bithumb.com', '').strip()
            if 'ë¹—ì¸' not in cleaned_query:
                cleaned_query = f"ë¹—ì¸ {cleaned_query}"
            processed_queries.append(cleaned_query)
        else:
            processed_queries.append(query)
    
    try:
        with DDGS() as ddgs:
            for query_idx, query in enumerate(processed_queries, 1):
                try:
                    results = list(ddgs.text(query, max_results=config.MAX_RESULTS_PER_QUERY))
                    
                    for result in results:
                        url = result.get("href", "")
                        if url and url not in seen_urls:
                            seen_urls.add(url)
                            all_results.append({
                                "title": result.get("title", ""),
                                "body": result.get("body", ""),
                                "href": url,
                            })
                    
                    if query_idx < len(processed_queries):
                        await asyncio.sleep(0.5)
                        
                except Exception as e:
                    logger.error(f"DuckDuckGo ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
    except Exception as e:
        logger.error(f"DuckDuckGo ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    logger.info(f"DuckDuckGo ê²€ìƒ‰ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼")
    return all_results


async def _get_price_from_api(coin_name: str, is_past_date: bool, requested_date):
    """ì‹œì„¸ APIì—ì„œ ê°€ê²© ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë‹¨ì¼ ì½”ì¸)"""
    try:
        if is_past_date and requested_date:
            # ê³¼ê±° ë‚ ì§œ: CoinGecko ìš°ì„ 
            try:
                from ...coingecko import coingecko_service
                price_data = await coingecko_service.get_price(coin_name, convert="krw", target_date=requested_date)
                if price_data:
                    return price_data, "coingecko_api"
            except ImportError:
                pass
            except Exception as e:
                logger.warning(f"CoinGecko API ì˜¤ë¥˜: {e}")
            
            # CoinMarketCap ì‹œë„
            try:
                from ...coinmarketcap import coinmarketcap_service
                price_data = await coinmarketcap_service.get_price(coin_name, convert="KRW", target_date=requested_date)
                if price_data:
                    return price_data, "coinmarketcap_api"
            except ImportError:
                pass
        else:
            # í˜„ì¬ ì‹œì„¸: CoinMarketCap
            try:
                from ...coinmarketcap import coinmarketcap_service
                price_data = await coinmarketcap_service.get_price(coin_name, convert="KRW", target_date=None)
                if price_data:
                    return price_data, "coinmarketcap_api"
            except ImportError:
                pass
    except Exception as e:
        logger.error(f"ì‹œì„¸ API ì˜¤ë¥˜: {e}")
    
    return None, None


async def _get_prices_from_api(coin_names: list, is_past_date: bool, requested_date):
    """ì‹œì„¸ APIì—ì„œ ì—¬ëŸ¬ ì½”ì¸ì˜ ê°€ê²© ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì²˜ë¦¬)"""
    import asyncio
    
    if not coin_names:
        return []
    
    # ëª¨ë“  ì½”ì¸ì„ ë³‘ë ¬ë¡œ ì¡°íšŒ
    tasks = [_get_price_from_api(coin_name, is_past_date, requested_date) for coin_name in coin_names]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    price_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.warning(f"ì½”ì¸ '{coin_names[i]}' ì¡°íšŒ ì‹¤íŒ¨: {result}")
            continue
        
        price_data, api_source = result
        if price_data:
            price_results.append((price_data, api_source, coin_names[i]))
    
    return price_results


def _extract_coin_names(user_message: str) -> list:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì—¬ëŸ¬ ì½”ì¸ëª… ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)"""
    coin_names = []
    try:
        from ...coinmarketcap import coinmarketcap_service
        
        # í•œêµ­ì–´ ì½”ì¸ëª… ì¶”ì¶œ (ëª¨ë“  ë§¤ì¹­)
        for coin_korean, coin_symbol in coinmarketcap_service.SYMBOL_MAPPING.items():
            if coin_korean in user_message:
                if coin_korean not in coin_names:
                    coin_names.append(coin_korean)
        
        # ì˜ì–´ ì‹¬ë³¼ ì¶”ì¶œ (ëª¨ë“  ë§¤ì¹­)
        symbol_matches = re.findall(r'\b([A-Z]{2,5})\b', user_message.upper())
        for symbol in symbol_matches:
            # ì•Œë ¤ì§„ ì‹¬ë³¼ì¸ì§€ í™•ì¸
            if symbol in coinmarketcap_service.SYMBOL_MAPPING.values():
                # ì‹¬ë³¼ì„ í•œêµ­ì–´ëª…ìœ¼ë¡œ ë³€í™˜
                for korean, eng_symbol in coinmarketcap_service.SYMBOL_MAPPING.items():
                    if eng_symbol == symbol and korean not in coin_names:
                        coin_names.append(korean)
                        break
    except ImportError:
        pass
    
    return coin_names if coin_names else []


def _extract_date_from_message(message: str):
    """ë©”ì‹œì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
    date_patterns = [
        r'(\d{4})-(\d{1,2})-(\d{1,2})',
        r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
        r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, message)
        if match:
            year, month, day = map(int, match.groups())
            try:
                kst = timezone(timedelta(hours=9))
                return datetime(year, month, day, tzinfo=kst)
            except ValueError:
                continue
    
    return None


@traceable(name="researcher", run_type="chain")
async def researcher(state: ChatState):
    """Researcher: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
    print("="*60, file=sys.stdout, flush=True)
    print("Researcher ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰", file=sys.stdout, flush=True)
    print("="*60, file=sys.stdout, flush=True)
    
    ensure_logger_setup()
    logger.info("="*60)
    logger.info("Researcher ë…¸ë“œ ì‹œì‘")
    
    search_queries = state.get("search_queries", [])
    current_messages = state.get("messages", [])
    user_messages = [msg for msg in current_messages if isinstance(msg, HumanMessage)]
    
    if not user_messages:
        return {"web_search_results": []}
    
    last_user_message = user_messages[-1].content
    msg_lower = last_user_message.lower()
    
    # ì‹œì„¸ ì§ˆë¬¸ ê°ì§€
    is_price_query = any(keyword in msg_lower for keyword in config.PRICE_KEYWORDS)
    
    # ë§¥ë½ ê¸°ë°˜ ì‹œì„¸ ì§ˆë¬¸ ê°ì§€
    if not is_price_query and len(user_messages) > 1:
        for prev_msg in user_messages[-3:-1]:
            prev_content = prev_msg.content.lower() if hasattr(prev_msg, 'content') else str(prev_msg).lower()
            if any(keyword in prev_content for keyword in config.PRICE_KEYWORDS):
                coin_only_patterns = ['ì€?', 'ëŠ”?', 'ë„?', 'ìš”?']
                if any(pattern in msg_lower for pattern in coin_only_patterns):
                    is_price_query = True
                    logger.info("âœ… ë§¥ë½ ê¸°ë°˜ ì‹œì„¸ ì§ˆë¬¸ ê°ì§€")
                    break
    
    # ë‚ ì§œ ì¶”ì¶œ
    requested_date = _extract_date_from_message(last_user_message)
    kst = timezone(timedelta(hours=9))
    today = datetime.now(kst).date()
    is_past_date = requested_date and requested_date.date() < today
    
    # 365ì¼ ì œí•œ í™•ì¸ (CoinGecko ë¬´ë£Œ í”Œëœ ì œí•œ)
    date_limit_exceeded = False
    if is_past_date and requested_date:
        days_diff = (today - requested_date.date()).days
        if days_diff > 365:
            date_limit_exceeded = True
            logger.info(f"âš ï¸ 365ì¼ ì œí•œ ì´ˆê³¼: ìš”ì²­ ë‚ ì§œê°€ {days_diff}ì¼ ì „ì…ë‹ˆë‹¤.")
    
    # ì‹œì„¸ ì§ˆë¬¸ì´ë©´ API ìš°ì„  ì‚¬ìš©
    if is_price_query:
        logger.info("âœ… ì‹œì„¸ ì§ˆë¬¸ ê°ì§€")
        print("[Researcher] âœ… ì‹œì„¸ ì§ˆë¬¸ ê°ì§€", file=sys.stdout, flush=True)
        
        coin_names = _extract_coin_names(last_user_message)
        
        if coin_names:
            logger.info(f"ì¶”ì¶œëœ ì½”ì¸: {coin_names}")
            print(f"[Researcher] ì¶”ì¶œëœ ì½”ì¸: {', '.join(coin_names)}", file=sys.stdout, flush=True)
            
            # 365ì¼ ì œí•œ ì´ˆê³¼ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
            if date_limit_exceeded:
                date_str = requested_date.strftime("%Yë…„ %mì›” %dì¼") if requested_date else "í•´ë‹¹ ë‚ ì§œ"
                limit_message = {
                    "title": "ê³¼ê±° ì‹œì„¸ ì¡°íšŒ ì œí•œ ì•ˆë‚´",
                    "snippet": f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ëŠ” ìµœê·¼ 365ì¼ ì´ë‚´ì˜ ê³¼ê±° ì‹œì„¸ë§Œ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                              f"ìš”ì²­í•˜ì‹  ë‚ ì§œ({date_str})ëŠ” í˜„ì¬ë¡œë¶€í„° 365ì¼ì„ ì´ˆê³¼í•˜ì—¬ ì¡°íšŒê°€ ì œí•œë©ë‹ˆë‹¤.\n\n"
                              f"ë” ì˜¤ë˜ëœ ê³¼ê±° ì‹œì„¸ ì¡°íšŒ ê¸°ëŠ¥ì€ ì¶”í›„ ì§€ì› ì˜ˆì •ì…ë‹ˆë‹¤. ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                    "url": "",
                    "source": "system_notice",
                    "score": 0.0,
                }
                logger.info("âš ï¸ 365ì¼ ì œí•œ ì´ˆê³¼ - ì‚¬ìš©ì ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜")
                print("[Researcher] âš ï¸ 365ì¼ ì œí•œ ì´ˆê³¼ - ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜", file=sys.stdout, flush=True)
                return {"web_search_results": [limit_message]}
            
            # ì—¬ëŸ¬ ì½”ì¸ ë³‘ë ¬ ì¡°íšŒ
            price_results = await _get_prices_from_api(coin_names, is_past_date, requested_date)
            
            if price_results:
                api_results = []
                date_info = f" ({requested_date.date()})" if is_past_date else ""
                
                for price_data, api_source, coin_name in price_results:
                    api_name = "CoinGecko" if "coingecko" in api_source else "CoinMarketCap"
                    
                    # ê°€ê²© í‘œì‹œ ìƒì„±
                    if price_data.get('price_krw') and price_data.get('price_usd', 0) > 0:
                        price_display_str = f"ğŸ’° í˜„ì¬ ê°€ê²©: {price_data['price_krw']:,.0f}ì› (${price_data['price_usd']:,.2f})"
                    elif price_data.get('price_krw'):
                        price_display_str = f"ğŸ’° í˜„ì¬ ê°€ê²©: {price_data['price_krw']:,.0f}ì›"
                    elif price_data.get('price_usd', 0) > 0:
                        price_display_str = f"ğŸ’° í˜„ì¬ ê°€ê²©: ${price_data['price_usd']:,.2f}"
                    else:
                        price_display_str = "ğŸ’° ê°€ê²© ì •ë³´ ì—†ìŒ"
                    
                    snippet = f"{price_data['name']} ({price_data['symbol']}) ì‹œì„¸{date_info}:\n\n{price_display_str}"
                    
                    if price_data.get('price_change_24h') is not None:
                        snippet += f"\nğŸ“Š 24ì‹œê°„ ë³€ë™ë¥ : {price_data['price_change_24h']:+.2f}%"
                    if price_data.get('market_cap'):
                        snippet += f"\nğŸ’¼ ì‹œê°€ì´ì•¡: ${price_data['market_cap']:,.0f}"
                    
                    snippet += f"\nğŸ• ì—…ë°ì´íŠ¸: {price_data['last_updated']}"
                    snippet += f"\n\nì¶œì²˜: {api_name}"
                    
                    api_result = {
                        "title": f"{price_data['name']} ì‹œì„¸{date_info} - {api_name}",
                        "snippet": snippet.strip(),
                        "url": f"https://coinmarketcap.com/currencies/{price_data['name'].lower().replace(' ', '-')}/",
                        "source": api_source,
                        "score": 0.95,
                    }
                    
                    api_results.append(api_result)
                    logger.info(f"âœ… {api_name} API ê²°ê³¼: {price_data['symbol']}")
                
                print(f"[Researcher] âœ… {len(api_results)}ê°œ ì½”ì¸ ì‹œì„¸ ì¡°íšŒ ì™„ë£Œ", file=sys.stdout, flush=True)
                
                return {"web_search_results": api_results}
            else:
                logger.warning("âš ï¸ API ì¡°íšŒ ì‹¤íŒ¨ - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
        else:
            logger.warning("âš ï¸ ì½”ì¸ëª… ì¶”ì¶œ ì‹¤íŒ¨ - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    if not search_queries:
        kst = timezone(timedelta(hours=9))
        current_year = datetime.now(kst).year
        
        if any(keyword in msg_lower for keyword in ['ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜']):
            search_queries = [
                f"ë¹—ì¸ ì§„í–‰ì¤‘ì¸ ì´ë²¤íŠ¸ {current_year}",
                "ë¹—ì¸ í˜„ì¬ í”„ë¡œëª¨ì…˜",
                "ë¹—ì¸ ì´ë²¤íŠ¸ ê³µì§€ì‚¬í•­"
            ]
        else:
            search_queries = [
                last_user_message,
                f"ë¹—ì¸ {last_user_message}",
                f"{last_user_message} ë¹—ì¸"
            ]
    
    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    web_search_results = []
    
    # Google ê²€ìƒ‰ ì‹œë„
    google_results, rate_limit_hit = await _search_with_google(search_queries)
    
    if google_results:
        for i, result in enumerate(google_results[:config.MAX_SEARCH_RESULTS], 1):
            web_search_results.append({
                "title": result.get("title", ""),
                "snippet": result.get("snippet", ""),
                "url": result.get("url", ""),
                "rank": i
            })
    
    # Google ì‹¤íŒ¨ ì‹œ DuckDuckGo
    if not web_search_results or rate_limit_hit:
        ddg_results = await _search_with_duckduckgo(search_queries)
        
        for i, result in enumerate(ddg_results[:config.MAX_SEARCH_RESULTS], 1):
            web_search_results.append({
                "title": result.get("title", ""),
                "snippet": result.get("body", ""),
                "url": result.get("href", ""),
                "rank": i
            })
    
    # ê²€ìƒ‰ ì™„ë£Œ ë©”ì‹œì§€
    search_summary = f"[ì›¹ ê²€ìƒ‰ ì™„ë£Œ]\n{len(web_search_results)}ê°œ ê²°ê³¼"
    researcher_message = AIMessage(content=search_summary)
    
    print(f"[Researcher] ì™„ë£Œ: {len(web_search_results)}ê°œ ê²°ê³¼", file=sys.stdout, flush=True)
    logger.info(f"Researcher ì™„ë£Œ: {len(web_search_results)}ê°œ ê²°ê³¼")
    logger.info("="*60)
    print("="*60, file=sys.stdout, flush=True)
    
    search_loop_count = state.get("search_loop_count", 0) + 1
    
    return {
        "web_search_results": web_search_results,
        "messages": current_messages + [researcher_message],
        "search_loop_count": search_loop_count,
        "summarized_results": []
    }

