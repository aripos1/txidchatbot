"""
Researcher ë…¸ë“œ - ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
"""
import re
import sys
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timezone, timedelta
import httpx
from langchain_core.messages import HumanMessage, AIMessage
from langsmith import traceable

from ...models import ChatState
from ...configuration import config
from ...utils import ensure_logger_setup

# ì„ íƒì  ì˜ì¡´ì„± (DuckDuckGo)
try:
    from ddgs import DDGS
except ImportError:
    try:
        from duckduckgo_search import DDGS
    except ImportError:
        DDGS = None

# ì„ íƒì  ì˜ì¡´ì„± (Tavily)
try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False
    TavilyClient = None

logger = logging.getLogger(__name__)


async def _search_single_google_query(client: httpx.AsyncClient, query: str, google_api_key: str, google_cx: str) -> tuple[list, bool]:
    """ë‹¨ì¼ Google ê²€ìƒ‰ ì¿¼ë¦¬ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    try:
        url = config.GOOGLE_SEARCH_API_URL
        params = {
            "key": google_api_key,
            "cx": google_cx,
            "q": query,
            "num": min(config.MAX_RESULTS_PER_QUERY, 10),
            "lr": "lang_ko",
        }
        
        response = await client.get(url, params=params)
        
        if response.status_code == 429:
            logger.warning(f"Google API í• ë‹¹ëŸ‰ ì´ˆê³¼: {query[:50]}")
            return [], True
        
        if response.status_code != 200:
            return [], False
        
        data = response.json()
        
        if "error" in data:
            error_code = data["error"].get("code", 0)
            if error_code == 429:
                return [], True
            return [], False
        
        results = []
        if "items" in data:
            for item in data.get("items", []):
                results.append({
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "url": item.get("link", ""),
                })
        
        return results, False
        
    except httpx.TimeoutException:
        logger.warning(f"Google ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query[:50]}")
        return [], False
    except Exception as e:
        logger.error(f"Google ê²€ìƒ‰ ì˜¤ë¥˜ ({query[:50]}): {e}")
        return [], False


async def _search_with_google(search_queries: list) -> tuple[list, bool]:
    """Google Custom Search APIë¡œ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)"""
    google_api_key = config.GOOGLE_API_KEY
    google_cx = config.GOOGLE_CX
    
    if not google_api_key or not google_cx:
        return [], False
    
    logger.info(f"Google ê²€ìƒ‰ ì‹œì‘: {len(search_queries)}ê°œ ì¿¼ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)")
    
    all_results = []
    seen_urls = set()
    rate_limit_hit = False
    
    async with httpx.AsyncClient(timeout=15.0) as client:
        # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        tasks = [
            _search_single_google_query(client, query, google_api_key, google_cx)
            for query in search_queries
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Google ì¿¼ë¦¬ ì˜¤ë¥˜ ({search_queries[i][:50]}): {result}")
                continue
            
            query_results, hit_rate_limit = result
            if hit_rate_limit:
                rate_limit_hit = True
            
            for item in query_results:
                url_link = item.get("url", "")
                if url_link and url_link not in seen_urls:
                    seen_urls.add(url_link)
                    all_results.append({
                        "title": item.get("title", ""),
                        "snippet": item.get("snippet", ""),
                        "url": url_link,
                    })
    
    logger.info(f"Google ê²€ìƒ‰ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼ (ë³‘ë ¬ ì²˜ë¦¬)")
    return all_results, rate_limit_hit


async def _search_single_ddg_query(query: str) -> list:
    """ë‹¨ì¼ DuckDuckGo ê²€ìƒ‰ ì¿¼ë¦¬ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    if DDGS is None:
        return []
    
    # site: ì¿¼ë¦¬ ë³€í™˜
    processed_query = query
    if 'site:bithumb.com' in query.lower():
        cleaned_query = query.lower().replace('site:bithumb.com', '').strip()
        if 'ë¹—ì¸' not in cleaned_query:
            cleaned_query = f"ë¹—ì¸ {cleaned_query}"
        processed_query = cleaned_query
    
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(processed_query, max_results=config.MAX_RESULTS_PER_QUERY))
            return results
    except Exception as e:
        logger.error(f"DuckDuckGo ì¿¼ë¦¬ ì˜¤ë¥˜ ({query[:50]}): {e}")
        return []


async def _search_with_duckduckgo(search_queries: list) -> list:
    """DuckDuckGoë¡œ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)"""
    if DDGS is None:
        logger.warning("DuckDuckGo ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
        return []
    
    logger.info(f"DuckDuckGo ê²€ìƒ‰ ì‹œì‘: {len(search_queries)}ê°œ ì¿¼ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)")
    
    all_results = []
    seen_urls = set()
    
    # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    tasks = [_search_single_ddg_query(query) for query in search_queries]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"DuckDuckGo ì¿¼ë¦¬ ì˜¤ë¥˜ ({search_queries[i][:50]}): {result}")
            continue
        
        for item in result:
            url = item.get("href", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                all_results.append({
                    "title": item.get("title", ""),
                    "body": item.get("body", ""),
                    "href": url,
                })
    
    logger.info(f"DuckDuckGo ê²€ìƒ‰ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼ (ë³‘ë ¬ ì²˜ë¦¬)")
    return all_results


async def _search_single_tavily_query(query: str, tavily_api_key: str) -> list:
    """ë‹¨ì¼ Tavily ê²€ìƒ‰ ì¿¼ë¦¬ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    if not TAVILY_AVAILABLE or not tavily_api_key:
        return []
    
    try:
        # TavilyClientëŠ” ë™ê¸°ì‹ì´ë¯€ë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        def _sync_search():
            client = TavilyClient(api_key=tavily_api_key)
            response = client.search(
                query=query,
                search_depth="basic",  # basic ë˜ëŠ” advanced
                max_results=config.MAX_RESULTS_PER_QUERY,
                include_answer=False,
                include_raw_content=False
            )
            return response.get("results", [])
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            results = await loop.run_in_executor(executor, _sync_search)
        
        return results
        
    except Exception as e:
        logger.error(f"Tavily ì¿¼ë¦¬ ì˜¤ë¥˜ ({query[:50]}): {e}")
        return []


async def _search_with_tavily(search_queries: list) -> list:
    """Tavilyë¡œ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)"""
    tavily_api_key = config.TAVILY_API_KEY
    
    if not TAVILY_AVAILABLE:
        logger.warning("Tavily ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
        return []
    
    if not tavily_api_key:
        logger.warning("Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    logger.info(f"Tavily ê²€ìƒ‰ ì‹œì‘: {len(search_queries)}ê°œ ì¿¼ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)")
    
    all_results = []
    seen_urls = set()
    
    # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    tasks = [_search_single_tavily_query(query, tavily_api_key) for query in search_queries]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"Tavily ì¿¼ë¦¬ ì˜¤ë¥˜ ({search_queries[i][:50]}): {result}")
            continue
        
        for item in result:
            url = item.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                all_results.append({
                    "title": item.get("title", ""),
                    "snippet": item.get("content", "") or item.get("snippet", ""),
                    "url": url,
                })
    
    logger.info(f"Tavily ê²€ìƒ‰ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼ (ë³‘ë ¬ ì²˜ë¦¬)")
    return all_results


async def _get_price_from_api(coin_name: str, is_past_date: bool, requested_date):
    """ì‹œì„¸ APIì—ì„œ ê°€ê²© ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë‹¨ì¼ ì½”ì¸)"""
    try:
        if is_past_date and requested_date:
            # ê³¼ê±° ë‚ ì§œ: CoinGecko ìš°ì„ 
            try:
                from ...coingecko import coingecko_service
                price_data = await coingecko_service.get_price(coin_name, convert="krw", target_date=requested_date)
                if price_data:
                    return price_data, "coingecko_api"
            except ImportError:
                pass
            except Exception as e:
                logger.warning(f"CoinGecko API ì˜¤ë¥˜: {e}")
            
            # CoinMarketCap ì‹œë„
            try:
                from ...coinmarketcap import coinmarketcap_service
                price_data = await coinmarketcap_service.get_price(coin_name, convert="KRW", target_date=requested_date)
                if price_data:
                    return price_data, "coinmarketcap_api"
            except ImportError:
                pass
        else:
            # í˜„ì¬ ì‹œì„¸: CoinMarketCap
            try:
                from ...coinmarketcap import coinmarketcap_service
                price_data = await coinmarketcap_service.get_price(coin_name, convert="KRW", target_date=None)
                if price_data:
                    return price_data, "coinmarketcap_api"
            except ImportError:
                pass
    except Exception as e:
        logger.error(f"ì‹œì„¸ API ì˜¤ë¥˜: {e}")
    
    return None, None


async def _get_prices_from_api(coin_names: list, is_past_date: bool, requested_date):
    """ì‹œì„¸ APIì—ì„œ ì—¬ëŸ¬ ì½”ì¸ì˜ ê°€ê²© ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì²˜ë¦¬)"""
    import asyncio
    
    if not coin_names:
        return []
    
    # ëª¨ë“  ì½”ì¸ì„ ë³‘ë ¬ë¡œ ì¡°íšŒ
    tasks = [_get_price_from_api(coin_name, is_past_date, requested_date) for coin_name in coin_names]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    price_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.warning(f"ì½”ì¸ '{coin_names[i]}' ì¡°íšŒ ì‹¤íŒ¨: {result}")
            continue
        
        price_data, api_source = result
        if price_data:
            price_results.append((price_data, api_source, coin_names[i]))
    
    return price_results


def _extract_coin_names(user_message: str) -> list:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì—¬ëŸ¬ ì½”ì¸ëª… ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
    
    ë„ì–´ì“°ê¸° ì²˜ë¦¬: "ë¹„íŠ¸ ì½”ì¸" â†’ "ë¹„íŠ¸ì½”ì¸"ìœ¼ë¡œ ì •ê·œí™”
    """
    coin_names = []
    try:
        from ...coinmarketcap import coinmarketcap_service
        
        # ë„ì–´ì“°ê¸° ì œê±° ë²„ì „ë„ ì²´í¬ (ì˜ˆ: "ë¹„íŠ¸ ì½”ì¸" â†’ "ë¹„íŠ¸ì½”ì¸")
        normalized_message = user_message.replace(" ", "")
        
        # í•œêµ­ì–´ ì½”ì¸ëª… ì¶”ì¶œ (ëª¨ë“  ë§¤ì¹­)
        # ì›ë³¸ ë©”ì‹œì§€ì™€ ì •ê·œí™”ëœ ë©”ì‹œì§€ ëª¨ë‘ ì²´í¬
        for coin_korean, coin_symbol in coinmarketcap_service.SYMBOL_MAPPING.items():
            # ì›ë³¸ ë©”ì‹œì§€ì—ì„œ ì²´í¬
            if coin_korean in user_message:
                if coin_korean not in coin_names:
                    coin_names.append(coin_korean)
            # ì •ê·œí™”ëœ ë©”ì‹œì§€ì—ì„œ ì²´í¬ (ë„ì–´ì“°ê¸° ì œê±°)
            elif coin_korean in normalized_message:
                if coin_korean not in coin_names:
                    coin_names.append(coin_korean)
            # ì—­ë°©í–¥ ì²´í¬: "ë¹„íŠ¸ ì½”ì¸"ì—ì„œ "ë¹„íŠ¸ì½”ì¸" ì°¾ê¸°
            elif coin_korean.replace(" ", "") in normalized_message:
                if coin_korean not in coin_names:
                    coin_names.append(coin_korean)
        
        # ì˜ì–´ ì‹¬ë³¼ ì¶”ì¶œ (ëª¨ë“  ë§¤ì¹­)
        symbol_matches = re.findall(r'\b([A-Z]{2,5})\b', user_message.upper())
        for symbol in symbol_matches:
            # ì•Œë ¤ì§„ ì‹¬ë³¼ì¸ì§€ í™•ì¸
            if symbol in coinmarketcap_service.SYMBOL_MAPPING.values():
                # ì‹¬ë³¼ì„ í•œêµ­ì–´ëª…ìœ¼ë¡œ ë³€í™˜
                for korean, eng_symbol in coinmarketcap_service.SYMBOL_MAPPING.items():
                    if eng_symbol == symbol and korean not in coin_names:
                        coin_names.append(korean)
                        break
        
        # ì¶”ê°€: ì¼ë°˜ì ì¸ ì½”ì¸ëª… íŒ¨í„´ ì²´í¬ (ì˜ˆ: "ë¹„íŠ¸ ì½”ì¸", "ì´ë” ë¦¬ì›€")
        common_patterns = {
            "ë¹„íŠ¸ ì½”ì¸": "ë¹„íŠ¸ì½”ì¸",
            "ì´ë” ë¦¬ì›€": "ì´ë”ë¦¬ì›€",
            "ë¦¬ í”Œ": "ë¦¬í”Œ",
            "ë„ì§€ ì½”ì¸": "ë„ì§€ì½”ì¸",
            "ì†” ë¼ë‚˜": "ì†”ë¼ë‚˜",
            "í´ì¹´ ë‹·": "í´ì¹´ë‹·",
            "ì²´ì¸ ë§í¬": "ì²´ì¸ë§í¬",
            "ìœ ë‹ˆ ìŠ¤ì™‘": "ìœ ë‹ˆìŠ¤ì™‘",
            "ì•„ë°œë€ ì²´": "ì•„ë°œë€ì²´",
            "í´ë¦¬ ê³¤": "í´ë¦¬ê³¤",
        }
        for spaced_name, correct_name in common_patterns.items():
            if spaced_name in user_message and correct_name not in coin_names:
                coin_names.append(correct_name)
        
    except ImportError:
        pass
    
    return coin_names if coin_names else []


def _extract_date_from_message(message: str):
    """ë©”ì‹œì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
    date_patterns = [
        r'(\d{4})-(\d{1,2})-(\d{1,2})',
        r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
        r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, message)
        if match:
            year, month, day = map(int, match.groups())
            try:
                kst = timezone(timedelta(hours=9))
                return datetime(year, month, day, tzinfo=kst)
            except ValueError:
                continue
    
    return None


@traceable(name="researcher", run_type="chain")
async def researcher(state: ChatState):
    """Researcher: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
    print("="*60, file=sys.stdout, flush=True)
    print("Researcher ë…¸ë“œ ì‹œì‘: ì›¹ ê²€ìƒ‰", file=sys.stdout, flush=True)
    print("="*60, file=sys.stdout, flush=True)
    
    ensure_logger_setup()
    logger.info("="*60)
    logger.info("Researcher ë…¸ë“œ ì‹œì‘")
    
    session_id = state.get("session_id", "default")
    search_queries = state.get("search_queries", [])
    current_messages = state.get("messages", [])
    user_messages = [msg for msg in current_messages if isinstance(msg, HumanMessage)]
    
    if not user_messages:
        return {
            "web_search_results": [],
            "session_id": session_id  # ì„¸ì…˜ ID ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
        }
    
    last_user_message = user_messages[-1].content
    msg_lower = last_user_message.lower()
    
    # ì‹œì„¸ ì§ˆë¬¸ ê°ì§€
    is_price_query = any(keyword in msg_lower for keyword in config.PRICE_KEYWORDS)
    
    # ë§¥ë½ ê¸°ë°˜ ì‹œì„¸ ì§ˆë¬¸ ê°ì§€
    if not is_price_query and len(user_messages) > 1:
        for prev_msg in user_messages[-3:-1]:
            prev_content = prev_msg.content.lower() if hasattr(prev_msg, 'content') else str(prev_msg).lower()
            if any(keyword in prev_content for keyword in config.PRICE_KEYWORDS):
                coin_only_patterns = ['ì€?', 'ëŠ”?', 'ë„?', 'ìš”?']
                if any(pattern in msg_lower for pattern in coin_only_patterns):
                    is_price_query = True
                    logger.info("âœ… ë§¥ë½ ê¸°ë°˜ ì‹œì„¸ ì§ˆë¬¸ ê°ì§€")
                    break
    
    # ë‚ ì§œ ì¶”ì¶œ
    requested_date = _extract_date_from_message(last_user_message)
    kst = timezone(timedelta(hours=9))
    today = datetime.now(kst).date()
    is_past_date = requested_date and requested_date.date() < today
    
    # 365ì¼ ì œí•œ í™•ì¸ (CoinGecko ë¬´ë£Œ í”Œëœ ì œí•œ)
    date_limit_exceeded = False
    if is_past_date and requested_date:
        days_diff = (today - requested_date.date()).days
        if days_diff > 365:
            date_limit_exceeded = True
            logger.info(f"âš ï¸ 365ì¼ ì œí•œ ì´ˆê³¼: ìš”ì²­ ë‚ ì§œê°€ {days_diff}ì¼ ì „ì…ë‹ˆë‹¤.")
    
    # ì‹œì„¸ ì§ˆë¬¸ì´ë©´ API ìš°ì„  ì‚¬ìš©
    if is_price_query:
        logger.info("âœ… ì‹œì„¸ ì§ˆë¬¸ ê°ì§€")
        print("[Researcher] âœ… ì‹œì„¸ ì§ˆë¬¸ ê°ì§€", file=sys.stdout, flush=True)
        
        coin_names = _extract_coin_names(last_user_message)
        
        if coin_names:
            logger.info(f"ì¶”ì¶œëœ ì½”ì¸: {coin_names}")
            print(f"[Researcher] ì¶”ì¶œëœ ì½”ì¸: {', '.join(coin_names)}", file=sys.stdout, flush=True)
            
            # 365ì¼ ì œí•œ ì´ˆê³¼ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
            if date_limit_exceeded:
                date_str = requested_date.strftime("%Yë…„ %mì›” %dì¼") if requested_date else "í•´ë‹¹ ë‚ ì§œ"
                limit_message = {
                    "title": "ê³¼ê±° ì‹œì„¸ ì¡°íšŒ ì œí•œ ì•ˆë‚´",
                    "snippet": f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ëŠ” ìµœê·¼ 365ì¼ ì´ë‚´ì˜ ê³¼ê±° ì‹œì„¸ë§Œ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                              f"ìš”ì²­í•˜ì‹  ë‚ ì§œ({date_str})ëŠ” í˜„ì¬ë¡œë¶€í„° 365ì¼ì„ ì´ˆê³¼í•˜ì—¬ ì¡°íšŒê°€ ì œí•œë©ë‹ˆë‹¤.\n\n"
                              f"ë” ì˜¤ë˜ëœ ê³¼ê±° ì‹œì„¸ ì¡°íšŒ ê¸°ëŠ¥ì€ ì¶”í›„ ì§€ì› ì˜ˆì •ì…ë‹ˆë‹¤. ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                    "url": "",
                    "source": "system_notice",
                    "score": 0.0,
                }
                logger.info("âš ï¸ 365ì¼ ì œí•œ ì´ˆê³¼ - ì‚¬ìš©ì ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜")
                print("[Researcher] âš ï¸ 365ì¼ ì œí•œ ì´ˆê³¼ - ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜", file=sys.stdout, flush=True)
                return {
                    "web_search_results": [limit_message],
                    "session_id": session_id  # ì„¸ì…˜ ID ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
                }
            
            # ì—¬ëŸ¬ ì½”ì¸ ë³‘ë ¬ ì¡°íšŒ
            price_results = await _get_prices_from_api(coin_names, is_past_date, requested_date)
            
            if price_results:
                api_results = []
                date_info = f" ({requested_date.date()})" if is_past_date else ""
                
                for price_data, api_source, coin_name in price_results:
                    api_name = "CoinGecko" if "coingecko" in api_source else "CoinMarketCap"
                    
                    # ê°€ê²© í‘œì‹œ ìƒì„±
                    if price_data.get('price_krw') and price_data.get('price_usd', 0) > 0:
                        price_display_str = f"ğŸ’° í˜„ì¬ ê°€ê²©: {price_data['price_krw']:,.0f}ì› (${price_data['price_usd']:,.2f})"
                    elif price_data.get('price_krw'):
                        price_display_str = f"ğŸ’° í˜„ì¬ ê°€ê²©: {price_data['price_krw']:,.0f}ì›"
                    elif price_data.get('price_usd', 0) > 0:
                        price_display_str = f"ğŸ’° í˜„ì¬ ê°€ê²©: ${price_data['price_usd']:,.2f}"
                    else:
                        price_display_str = "ğŸ’° ê°€ê²© ì •ë³´ ì—†ìŒ"
                    
                    snippet = f"{price_data['name']} ({price_data['symbol']}) ì‹œì„¸{date_info}:\n\n{price_display_str}"
                    
                    if price_data.get('price_change_24h') is not None:
                        snippet += f"\nğŸ“Š 24ì‹œê°„ ë³€ë™ë¥ : {price_data['price_change_24h']:+.2f}%"
                    if price_data.get('market_cap'):
                        snippet += f"\nğŸ’¼ ì‹œê°€ì´ì•¡: ${price_data['market_cap']:,.0f}"
                    
                    snippet += f"\nğŸ• ì—…ë°ì´íŠ¸: {price_data['last_updated']}"
                    snippet += f"\n\nì¶œì²˜: {api_name}"
                    
                    api_result = {
                        "title": f"{price_data['name']} ì‹œì„¸{date_info} - {api_name}",
                        "snippet": snippet.strip(),
                        "url": f"https://coinmarketcap.com/currencies/{price_data['name'].lower().replace(' ', '-')}/",
                        "source": api_source,
                        "score": 0.95,
                    }
                    
                    api_results.append(api_result)
                    logger.info(f"âœ… {api_name} API ê²°ê³¼: {price_data['symbol']}")
                
                print(f"[Researcher] âœ… {len(api_results)}ê°œ ì½”ì¸ ì‹œì„¸ ì¡°íšŒ ì™„ë£Œ", file=sys.stdout, flush=True)
                
                return {
                    "web_search_results": api_results,
                    "session_id": session_id  # ì„¸ì…˜ ID ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
                }
            else:
                logger.warning("âš ï¸ API ì¡°íšŒ ì‹¤íŒ¨ - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
        else:
            logger.warning("âš ï¸ ì½”ì¸ëª… ì¶”ì¶œ ì‹¤íŒ¨ - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    if not search_queries:
        kst = timezone(timedelta(hours=9))
        current_year = datetime.now(kst).year
        
        if any(keyword in msg_lower for keyword in ['ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜']):
            search_queries = [
                f"ë¹—ì¸ ì§„í–‰ì¤‘ì¸ ì´ë²¤íŠ¸ {current_year}",
                "ë¹—ì¸ í˜„ì¬ í”„ë¡œëª¨ì…˜",
                "ë¹—ì¸ ì´ë²¤íŠ¸ ê³µì§€ì‚¬í•­"
            ]
        else:
            search_queries = [
                last_user_message,
                f"ë¹—ì¸ {last_user_message}",
                f"{last_user_message} ë¹—ì¸"
            ]
    
    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ë³‘ë ¬ ì²˜ë¦¬: Google, DuckDuckGo, Tavily ë™ì‹œ ì‹¤í–‰)
    web_search_results = []
    
    # ì´ì „ ê²€ìƒ‰ì—ì„œ Google API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸
    google_rate_limit_hit = state.get("google_rate_limit_hit", False)
    
    if google_rate_limit_hit:
        logger.info("âš ï¸ ì´ì „ ê²€ìƒ‰ì—ì„œ Google API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - Google ê²€ìƒ‰ ê±´ë„ˆë›°ê¸°")
        print("[Researcher] âš ï¸ Google API í• ë‹¹ëŸ‰ ì´ˆê³¼ - Google ê²€ìƒ‰ ê±´ë„ˆë›°ê¸°", file=sys.stdout, flush=True)
        google_results = []
        rate_limit_hit = True
        # DuckDuckGoì™€ Tavilyë§Œ ì‹¤í–‰
        duckduckgo_task = _search_with_duckduckgo(search_queries)
        tavily_task = _search_with_tavily(search_queries)
        
        results = await asyncio.gather(
            duckduckgo_task,
            tavily_task,
            return_exceptions=True
        )
        
        # DuckDuckGo ê²°ê³¼ ì²˜ë¦¬
        if isinstance(results[0], Exception):
            logger.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {results[0]}")
            ddg_results = []
        else:
            ddg_results = results[0]
        
        # Tavily ê²°ê³¼ ì²˜ë¦¬
        if isinstance(results[1], Exception):
            logger.error(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {results[1]}")
            tavily_results = []
        else:
            tavily_results = results[1]
    else:
        logger.info("ğŸ”€ ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘: Google + DuckDuckGo + Tavily ë™ì‹œ ì‹¤í–‰")
        print("[Researcher] ğŸ”€ ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘: Google + DuckDuckGo + Tavily", file=sys.stdout, flush=True)
        
        # Google, DuckDuckGo, Tavily ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        google_task = _search_with_google(search_queries)
        duckduckgo_task = _search_with_duckduckgo(search_queries)
        tavily_task = _search_with_tavily(search_queries)
        
        results = await asyncio.gather(
            google_task,
            duckduckgo_task,
            tavily_task,
            return_exceptions=True
        )
        
        # Google ê²°ê³¼ ì²˜ë¦¬
        if isinstance(results[0], Exception):
            logger.error(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {results[0]}")
            google_results = []
            rate_limit_hit = False
        else:
            google_results, rate_limit_hit = results[0]
        
        # DuckDuckGo ê²°ê³¼ ì²˜ë¦¬
        if isinstance(results[1], Exception):
            logger.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {results[1]}")
            ddg_results = []
        else:
            ddg_results = results[1]
        
        # Tavily ê²°ê³¼ ì²˜ë¦¬
        if isinstance(results[2], Exception):
            logger.error(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {results[2]}")
            tavily_results = []
        else:
            tavily_results = results[2]
    
    # Google ê²°ê³¼ ì¶”ê°€
    if google_results:
        for i, result in enumerate(google_results[:config.MAX_SEARCH_RESULTS], 1):
            web_search_results.append({
                "title": result.get("title", ""),
                "snippet": result.get("snippet", ""),
                "url": result.get("url", ""),
                "rank": i,
                "source": "google"
            })
    
    if ddg_results:
        seen_urls = {r.get("url", "") for r in web_search_results}
        rank_offset = len(web_search_results)
        
        for i, result in enumerate(ddg_results[:config.MAX_SEARCH_RESULTS], 1):
            url = result.get("href", "") or result.get("url", "")
            if url and url not in seen_urls:
                web_search_results.append({
                    "title": result.get("title", ""),
                    "snippet": result.get("body", "") or result.get("snippet", ""),
                    "url": url,
                    "rank": rank_offset + i,
                    "source": "duckduckgo"
                })
    
    # Tavily ê²°ê³¼ ì¶”ê°€
    if tavily_results:
        seen_urls = {r.get("url", "") for r in web_search_results}
        rank_offset = len(web_search_results)
        
        for i, result in enumerate(tavily_results[:config.MAX_SEARCH_RESULTS], 1):
            url = result.get("url", "")
            if url and url not in seen_urls:
                web_search_results.append({
                    "title": result.get("title", ""),
                    "snippet": result.get("snippet", ""),
                    "url": url,
                    "rank": rank_offset + i,
                    "source": "tavily"
                })
    
    # ê²€ìƒ‰ ì™„ë£Œ ë©”ì‹œì§€
    search_summary = f"[ì›¹ ê²€ìƒ‰ ì™„ë£Œ]\n{len(web_search_results)}ê°œ ê²°ê³¼"
    researcher_message = AIMessage(content=search_summary)
    
    print(f"[Researcher] ì™„ë£Œ: {len(web_search_results)}ê°œ ê²°ê³¼", file=sys.stdout, flush=True)
    logger.info(f"Researcher ì™„ë£Œ: {len(web_search_results)}ê°œ ê²°ê³¼")
    logger.info("="*60)
    print("="*60, file=sys.stdout, flush=True)
    
    search_loop_count = state.get("search_loop_count", 0) + 1
    
    # Google API í• ë‹¹ëŸ‰ ì´ˆê³¼ê°€ ë°œìƒí–ˆìœ¼ë©´ stateì— ì €ì¥ (ë‹¤ìŒ ê²€ìƒ‰ì—ì„œ Google ê±´ë„ˆë›°ê¸°)
    if rate_limit_hit:
        logger.info("âš ï¸ Google API í• ë‹¹ëŸ‰ ì´ˆê³¼ - ë‹¤ìŒ ê²€ìƒ‰ì—ì„œ Google ê±´ë„ˆë›°ê¸°")
    
    return {
        "web_search_results": web_search_results,
        "messages": current_messages + [researcher_message],
        "search_loop_count": search_loop_count,
        "google_rate_limit_hit": rate_limit_hit,  # Google API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—¬ë¶€ ì €ì¥
        "summarized_results": [],
        "session_id": session_id  # ì„¸ì…˜ ID ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
    }

